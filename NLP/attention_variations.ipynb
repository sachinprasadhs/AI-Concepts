{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is Einsum ?\n\n* Let's say we want to multiply two matrices `A` and `B` followed by calculating the sum of each column resulting in a vector `C`. Using Einstein summation notation, we can write this as\n\n$$c_{j} = \\sum_{i} \\sum_{j} A_{ik}B_{kj} = A_{ik}B_{kj}$$\n","metadata":{}},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport optax","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-25T23:02:53.565095Z","iopub.execute_input":"2024-04-25T23:02:53.565528Z","iopub.status.idle":"2024-04-25T23:02:54.692505Z","shell.execute_reply.started":"2024-04-25T23:02:53.565492Z","shell.execute_reply":"2024-04-25T23:02:54.691362Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"a = jnp.arange(6).reshape((2, 3))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:35:13.248681Z","iopub.execute_input":"2024-04-25T21:35:13.249079Z","iopub.status.idle":"2024-04-25T21:35:13.274393Z","shell.execute_reply.started":"2024-04-25T21:35:13.249048Z","shell.execute_reply":"2024-04-25T21:35:13.273199Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(a)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:37:25.515368Z","iopub.execute_input":"2024-04-25T21:37:25.515831Z","iopub.status.idle":"2024-04-25T21:37:25.523072Z","shell.execute_reply.started":"2024-04-25T21:37:25.515798Z","shell.execute_reply":"2024-04-25T21:37:25.521712Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[[0 1 2]\n [3 4 5]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(b)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:37:31.480604Z","iopub.execute_input":"2024-04-25T21:37:31.481134Z","iopub.status.idle":"2024-04-25T21:37:31.488301Z","shell.execute_reply.started":"2024-04-25T21:37:31.481030Z","shell.execute_reply":"2024-04-25T21:37:31.486488Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[[0 1]\n [2 3]\n [4 5]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Transpose","metadata":{}},{"cell_type":"code","source":"jnp.einsum(\"ij->ji\", a) # basically the notation is saying in abstract way to change each element from i,j position to j,i -> this is what expected in transpose","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:34:32.675923Z","iopub.execute_input":"2024-04-25T21:34:32.676296Z","iopub.status.idle":"2024-04-25T21:34:32.717806Z","shell.execute_reply.started":"2024-04-25T21:34:32.676269Z","shell.execute_reply":"2024-04-25T21:34:32.716593Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Array([[0, 3],\n       [1, 4],\n       [2, 5]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Matrix Sum","metadata":{}},{"cell_type":"code","source":"jnp.einsum(\"ij->\", a) # matrix sum is a single scalar output so there is no notation after `->` which means collapse all columns and rows","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:35:48.224279Z","iopub.execute_input":"2024-04-25T21:35:48.224661Z","iopub.status.idle":"2024-04-25T21:35:48.274922Z","shell.execute_reply.started":"2024-04-25T21:35:48.224632Z","shell.execute_reply":"2024-04-25T21:35:48.273549Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Array(15, dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Column Sum","metadata":{}},{"cell_type":"code","source":"jnp.einsum(\"ij->j\", a) # matrix sum column wise so the output has only how many dimensions in the column so rows are collapsed.","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:36:26.079801Z","iopub.execute_input":"2024-04-25T21:36:26.080911Z","iopub.status.idle":"2024-04-25T21:36:26.089688Z","shell.execute_reply.started":"2024-04-25T21:36:26.080872Z","shell.execute_reply":"2024-04-25T21:36:26.088137Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Array([3, 5, 7], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Row Sum","metadata":{}},{"cell_type":"code","source":"jnp.einsum(\"ij->i\", a)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:38:11.397418Z","iopub.execute_input":"2024-04-25T21:38:11.397863Z","iopub.status.idle":"2024-04-25T21:38:11.406997Z","shell.execute_reply.started":"2024-04-25T21:38:11.397829Z","shell.execute_reply":"2024-04-25T21:38:11.405768Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Array([ 3, 12], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Matrix-Vector Multiplication\n\n\n","metadata":{}},{"cell_type":"code","source":"b = jnp.arange(3) # (3, )\njnp.einsum(\"ij,j->i\", a, b) # same as jnp.dot(a, b)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:41:59.315231Z","iopub.execute_input":"2024-04-25T21:41:59.315631Z","iopub.status.idle":"2024-04-25T21:41:59.356844Z","shell.execute_reply.started":"2024-04-25T21:41:59.315604Z","shell.execute_reply":"2024-04-25T21:41:59.355467Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Array([ 5, 14], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Matrix-Matrix Multiplication","metadata":{}},{"cell_type":"code","source":"a = jnp.arange(6).reshape(2, 3)\nb = jnp.arange(15).reshape(3, 5)\njnp.einsum('ik,kj->ij', a, b)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:50:35.514760Z","iopub.execute_input":"2024-04-25T21:50:35.515158Z","iopub.status.idle":"2024-04-25T21:50:35.525169Z","shell.execute_reply.started":"2024-04-25T21:50:35.515127Z","shell.execute_reply":"2024-04-25T21:50:35.523896Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Array([[ 25,  28,  31,  34,  37],\n       [ 70,  82,  94, 106, 118]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"jnp.einsum('ik,kj->j', a, b) # matrix multiplication plus columns wise sum","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:51:04.149387Z","iopub.execute_input":"2024-04-25T21:51:04.149850Z","iopub.status.idle":"2024-04-25T21:51:04.159296Z","shell.execute_reply.started":"2024-04-25T21:51:04.149815Z","shell.execute_reply":"2024-04-25T21:51:04.157815Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"Array([ 95, 110, 125, 140, 155], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"jnp.einsum('ik,kj->i', a, b) # matrix multiplication plus row wise sum","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:51:19.555929Z","iopub.execute_input":"2024-04-25T21:51:19.556357Z","iopub.status.idle":"2024-04-25T21:51:19.565239Z","shell.execute_reply.started":"2024-04-25T21:51:19.556322Z","shell.execute_reply":"2024-04-25T21:51:19.563851Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"Array([155, 470], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Dot Product","metadata":{}},{"cell_type":"code","source":"a = jnp.arange(3)\nb = jnp.arange(3,6)\njnp.einsum('i,i->', a, b) # notation is basically i is index of a matrix and the i index in b multiplied index wise","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:56:33.449682Z","iopub.execute_input":"2024-04-25T21:56:33.450137Z","iopub.status.idle":"2024-04-25T21:56:33.503060Z","shell.execute_reply.started":"2024-04-25T21:56:33.450106Z","shell.execute_reply":"2024-04-25T21:56:33.501676Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Array(14, dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Element Wise Multiplication of Two Matrices and the summation","metadata":{}},{"cell_type":"code","source":"a = jnp.arange(6).reshape(2, 3)\nb = jnp.arange(6,12).reshape(2, 3)\njnp.einsum('ij,ij->ij', a, b)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:07:52.010268Z","iopub.execute_input":"2024-04-25T22:07:52.010638Z","iopub.status.idle":"2024-04-25T22:07:52.021702Z","shell.execute_reply.started":"2024-04-25T22:07:52.010608Z","shell.execute_reply":"2024-04-25T22:07:52.019959Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"Array([[ 0,  7, 16],\n       [27, 40, 55]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"jnp.einsum('ij,ij->', a, b)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:07:57.924720Z","iopub.execute_input":"2024-04-25T22:07:57.925119Z","iopub.status.idle":"2024-04-25T22:07:57.933343Z","shell.execute_reply.started":"2024-04-25T22:07:57.925090Z","shell.execute_reply":"2024-04-25T22:07:57.932180Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"Array(145, dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Outer Product\n\nGiven two vectors of size $m \\times 1$ and $n \\times 1$respectively\n\n$$ u = \\begin{bmatrix}\n  u_{1} \\\\\n  u_{2} \\\\\n  u_{3} \\\\\n  u_{4} \\\\\n\\end{bmatrix}\n\\,\nv = \\begin{bmatrix}\n  v_{1} \\\\\n  v_{2} \\\\\n  v_{3} \\\\\n\\end{bmatrix}$$\n$$$$\n$$ u \\otimes  v = uv^{T} = \\begin{bmatrix}\n  u_{1}v_{1} & u_{1}v_{2} & u_{1}v_{3}\\\\\n  u_{2}v_{1} & u_{2}v_{2} & u_{2}v_{3}\\\\\n  u_{3}v_{1} & u_{3}v_{2} & u_{3}v_{3}\\\\\n  u_{4}v_{1} & u_{4}v_{2} & u_{4}v_{3}\\\\\n\\end{bmatrix}$$","metadata":{}},{"cell_type":"code","source":"a = jnp.arange(3)\nb = jnp.arange(3,7)\njnp.einsum('i,j->ij', a, b)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:08:33.298948Z","iopub.execute_input":"2024-04-25T22:08:33.299327Z","iopub.status.idle":"2024-04-25T22:08:33.333892Z","shell.execute_reply.started":"2024-04-25T22:08:33.299299Z","shell.execute_reply":"2024-04-25T22:08:33.332573Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"Array([[ 0,  0,  0,  0],\n       [ 3,  4,  5,  6],\n       [ 6,  8, 10, 12]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Dot-Product Attention\n\n$$ Attention(q, K, V) = softmax(\\dfrac {q \\cdot K^{T}}{\\sqrt d_{k}})V$$","metadata":{}},{"cell_type":"code","source":"def dot_product_attention(q, K, V):\n    \"\"\" \n    Dot−Product Attention on one query.\n    Args :\n        q : a vector with shape [k]\n        K: a matrix with shape [m, k]\n        V: a ma trix with shape [m, v]\n    Returns :\n        y : a vector with shape [v]\n    \"\"\"\n    \n    logits = jnp.einsum(\"k,mk->m\", q, K)\n    weights = jax.nn.softmax(logits)\n    return jnp.einsum(\"m,mv->v\", weights, V)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:15:42.354714Z","iopub.execute_input":"2024-04-25T23:15:42.355147Z","iopub.status.idle":"2024-04-25T23:15:42.361769Z","shell.execute_reply.started":"2024-04-25T23:15:42.355113Z","shell.execute_reply":"2024-04-25T23:15:42.360649Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"key = jax.random.PRNGKey(13)\nd_model = 512\nseq_len = 128\nnum_heads = 8","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:12:17.232531Z","iopub.execute_input":"2024-04-26T00:12:17.233013Z","iopub.status.idle":"2024-04-26T00:12:17.240909Z","shell.execute_reply.started":"2024-04-26T00:12:17.232980Z","shell.execute_reply":"2024-04-26T00:12:17.239513Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"q = jax.random.uniform(key, shape=(d_model // num_heads,)) # One token with embedding size of 384 projected to a 64 one of head\nK = jax.random.uniform(key, shape=(seq_len, d_model // num_heads)) # consider a sentence with max of 128 sequence length and each token with 384 dimension projected to a 64 one of head\nV = jax.random.uniform(key, shape=(seq_len, d_model // num_heads)) # consider a sentence with max of 128 sequence length and each token with 384 dimension projected to a 64 one of head","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:12:23.368444Z","iopub.execute_input":"2024-04-26T00:12:23.368847Z","iopub.status.idle":"2024-04-26T00:12:23.377171Z","shell.execute_reply.started":"2024-04-26T00:12:23.368817Z","shell.execute_reply":"2024-04-26T00:12:23.376016Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"print(f\"q: {q.shape}\")\nprint(f\"K: {K.shape}\")\nprint(f\"V: {V.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:45:42.996616Z","iopub.execute_input":"2024-04-25T23:45:42.997061Z","iopub.status.idle":"2024-04-25T23:45:43.003341Z","shell.execute_reply.started":"2024-04-25T23:45:42.997028Z","shell.execute_reply":"2024-04-25T23:45:43.002001Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"q: (64,)\nK: (128, 64)\nV: (128, 64)\n","output_type":"stream"}]},{"cell_type":"code","source":"attention_scores = dot_product_attention(q, K, V)\nattention_scores.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:45:44.799582Z","iopub.execute_input":"2024-04-25T23:45:44.799982Z","iopub.status.idle":"2024-04-25T23:45:44.810710Z","shell.execute_reply.started":"2024-04-25T23:45:44.799951Z","shell.execute_reply":"2024-04-25T23:45:44.809315Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"(128,)\n","output_type":"stream"},{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"(64,)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Scaled Dot-Product Attention\n\n$$ Attention(Q, K, V) = softmax(\\dfrac {Q \\cdot K^{T}}{\\sqrt d_{k}})V$$","metadata":{}},{"cell_type":"code","source":"def scaled_dot_product_attention(Q, K, V):\n    \"\"\" \n    Scaled Dot−Product Attention on maximum sequence length of queries.\n    Args :\n        Q: a matrix with shape [m, q]\n        K: a matrix with shape [m, k]\n        V: a matrix with shape [m, v]\n    Returns :\n        y : a vector with shape [m, v]\n    \"\"\"\n    \n    logits = jnp.einsum(\"mq,mk->qk\", Q, K)\n    weights = jax.nn.softmax(logits)\n    return jnp.einsum(\"vv,mv->mv\", weights, V)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:45:46.852018Z","iopub.execute_input":"2024-04-25T23:45:46.852449Z","iopub.status.idle":"2024-04-25T23:45:46.859420Z","shell.execute_reply.started":"2024-04-25T23:45:46.852416Z","shell.execute_reply":"2024-04-25T23:45:46.858449Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"Q = jax.random.uniform(key, shape=(seq_len, d_model // num_heads)) # consider a sentence with max of 128 sequence length and each token with 384 dimension projected to a 64 one of head\nK = jax.random.uniform(key, shape=(seq_len, d_model // num_heads)) # consider a sentence with max of 128 sequence length and each token with 384 dimension projected to a 64 one of head\nV = jax.random.uniform(key, shape=(seq_len, d_model // num_heads)) # consider a sentence with max of 128 sequence length and each token with 384 dimension projected to a 64 one of head","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:12:30.460393Z","iopub.execute_input":"2024-04-26T00:12:30.460833Z","iopub.status.idle":"2024-04-26T00:12:30.470767Z","shell.execute_reply.started":"2024-04-26T00:12:30.460799Z","shell.execute_reply":"2024-04-26T00:12:30.469502Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"attention_scores = scaled_dot_product_attention(Q, K, V)\nattention_scores.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-25T23:45:50.320180Z","iopub.execute_input":"2024-04-25T23:45:50.320566Z","iopub.status.idle":"2024-04-25T23:45:50.330621Z","shell.execute_reply.started":"2024-04-25T23:45:50.320536Z","shell.execute_reply":"2024-04-25T23:45:50.329277Z"},"trusted":true},"execution_count":107,"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"(128, 64)"},"metadata":{}}]},{"cell_type":"code","source":"def multi_head_attention(\n    X, M, P_q, P_k, P_v, P_o):\n    \"\"\"Multi-head Attention on maximum sequence length of queries.\n    Args:\n        X: a matrix with shape of [n, d]\n        M: a matrix with shape of [m, d]\n        P_q: a tensor with shape of [h, d, k]\n        P_k: a tensor with shape of [h, d, k]\n        P_v: a tensor with shape of [h, d, v]\n        P_o:  a tensor with shape of [h, d, v]\n    Returns:\n        y: a tensor with shape of [n, d]\n    \"\"\"\n    Q = jnp.einsum(\"nd,hdk->hnk\", X, P_q)\n    K = jnp.einsum(\"md,hdk->hmk\", M, P_k)\n    V = jnp.einsum(\"md,hdv->hmv\", M, P_v)\n    \n    logits = jnp.einsum(\"hnk,hmk->hnm\", Q, K)\n    weights = jax.nn.softmax(logits)\n    O = jnp.einsum(\"hnm,hmv->hnv\", weights, V)\n    Y = jnp.einsum(\"hnv,hdv->nd\", O, P_o)\n    \n    return Y","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:19:40.296215Z","iopub.execute_input":"2024-04-26T00:19:40.296661Z","iopub.status.idle":"2024-04-26T00:19:40.304433Z","shell.execute_reply.started":"2024-04-26T00:19:40.296632Z","shell.execute_reply":"2024-04-26T00:19:40.303060Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"X = jax.random.uniform(key, shape=(seq_len, d_model)) # Input tensor with seq_len and embedding size \nM = jax.random.uniform(key, shape=(seq_len, d_model)) \n\n# Basically this is the dense layer weights, so when input is passed through dense layer we will get an outputs shape (heads, seql_len, head_size) projection\nP_q = jax.random.uniform(key, shape=(seq_len, d_model, d_model // num_heads)) # Projection of input tensor embeddings to each head size which is d_model // num_heads\nP_k = jax.random.uniform(key, shape=(seq_len, d_model, d_model // num_heads))\nP_v = jax.random.uniform(key, shape=(seq_len, d_model, d_model // num_heads))\nP_o = jax.random.uniform(key, shape=(seq_len, d_model, d_model // num_heads))\n\nattention_scores = multi_head_attention(X, M, P_q, P_k, P_v, P_o)\nattention_scores.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:20:21.520622Z","iopub.execute_input":"2024-04-26T00:20:21.522192Z","iopub.status.idle":"2024-04-26T00:20:21.766334Z","shell.execute_reply.started":"2024-04-26T00:20:21.522141Z","shell.execute_reply":"2024-04-26T00:20:21.765093Z"},"trusted":true},"execution_count":125,"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"(128, 512)"},"metadata":{}}]},{"cell_type":"code","source":"def batched_multi_head_attention(\n    X, M, P_q, P_k, P_v, P_o):\n    \"\"\"Multi-head Attention on maximum sequence length of queries.\n    Args:\n        X: a matrix with shape of [b, n, d]\n        M: a matrix with shape of [b, m, d]\n        P_q: a tensor with shape of [b, h, d, k]\n        P_k: a tensor with shape of [b, h, d, k]\n        P_v: a tensor with shape of [b, h, d, v]\n        P_o:  a tensor with shape of [b, h, d, v]\n    Returns:\n        y: a tensor with shape of [b, n, d]\n    \"\"\"\n    Q = jnp.einsum(\"bnd,bhdk->bhnk\", X, P_q)\n    K = jnp.einsum(\"bmd,bhdk->bhmk\", M, P_k)\n    V = jnp.einsum(\"bmd,bhdv->bhmv\", M, P_v)\n    \n    logits = jnp.einsum(\"bhnk,bhmk->bhnm\", Q, K)\n    weights = jax.nn.softmax(logits)\n    O = jnp.einsum(\"bhnm,bhmv->bhnv\", weights, V)\n    Y = jnp.einsum(\"bhnv,bhdv->bnd\", O, P_o)\n    \n    return Y","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:21:47.743754Z","iopub.execute_input":"2024-04-26T00:21:47.744140Z","iopub.status.idle":"2024-04-26T00:21:47.752071Z","shell.execute_reply.started":"2024-04-26T00:21:47.744110Z","shell.execute_reply":"2024-04-26T00:21:47.750589Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"X = jax.random.uniform(key, shape=(batch_size, seq_len, d_model)) # Input tensor with seq_len and embedding size \nM = jax.random.uniform(key, shape=(batch_size, seq_len, d_model)) \n\n# Basically this is the dense layer weights, so when input is passed through dense layer we will get an outputs shape (heads, seql_len, head_size) projection\nP_q = jax.random.uniform(key, shape=(batch_size, seq_len, d_model, d_model // num_heads)) # Projection of input tensor embeddings to each head size which is d_model // num_heads\nP_k = jax.random.uniform(key, shape=(batch_size, seq_len, d_model, d_model // num_heads))\nP_v = jax.random.uniform(key, shape=(batch_size, seq_len, d_model, d_model // num_heads))\nP_o = jax.random.uniform(key, shape=(batch_size, seq_len, d_model, d_model // num_heads))\n\nattention_scores = batched_multi_head_attention(X, M, P_q, P_k, P_v, P_o)\nattention_scores.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:21:53.628082Z","iopub.execute_input":"2024-04-26T00:21:53.628583Z","iopub.status.idle":"2024-04-26T00:22:05.778636Z","shell.execute_reply.started":"2024-04-26T00:21:53.628549Z","shell.execute_reply":"2024-04-26T00:22:05.777286Z"},"trusted":true},"execution_count":129,"outputs":[{"execution_count":129,"output_type":"execute_result","data":{"text/plain":"(32, 128, 512)"},"metadata":{}}]},{"cell_type":"markdown","source":"# References:\n\n1. https://rockt.github.io/2018/04/30/einsum\n2. https://arxiv.org/pdf/1706.03762","metadata":{}}]}