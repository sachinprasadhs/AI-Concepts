{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9QvfJiVclUJ"
      },
      "source": [
        "\n",
        "# Linear Regression\n",
        "**Assumptions**\n",
        "\n",
        "1. Linearity: The relationship between X and the mean of Y is linear.\n",
        "2. Homoscedasticity: The variance of residual is the same for any value of X.\n",
        "3. Independence: Observations are independent of each other.\n",
        "4. Normality: For any fixed value of X, Y is normally distributed.\n",
        "\n",
        "\n",
        "**Ref:**\n",
        "1. [Math Differential Calculus](https://colab.research.google.com/github/ageron/handson-ml2/blob/master/math_differential_calculus.ipynb#scrollTo=nwzF_EXtqqe1)\n",
        "\n",
        "2. [Hands-On Machine Learning](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1098125975/ref=asc_df_1098125975/?tag=hyprod-20&linkCode=df0&hvadid=564681728094&hvpos=&hvnetw=g&hvrand=12536021134787862812&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9061321&hvtargid=pla-1651497364252&psc=1&mcid=ee20b34166b93f94b9a88a727e2204b3&gclid=CjwKCAjwt-OwBhBnEiwAgwzrUvy6fyRVoCj9LZC1E4kgyL5KvZKoFNJFSWa2nRThyPByYb_YQCTPERoCVQsQAvD_BwE)\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/sachinprasadhs/AI-Concepts/blob/master/Common%20Concepts/Linear%20Regression/linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLwTjXles9dY"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "79bnrYHos7J-"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4ilyJN8tFQ4"
      },
      "source": [
        "## Generate Random Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tgUhw7THtNjF"
      },
      "outputs": [],
      "source": [
        "num_samples = 1000\n",
        "num_features = 5\n",
        "key = jax.random.PRNGKey(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p-DU_5BtHur",
        "outputId": "6c6ff723-b8dc-471f-a795-1d14ce118fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Bias: [-6]\n",
            "Random Coefficients: [-3 -1 -3 -6 -6]\n",
            "Y = -6 + -3 * X1 + -1 * X2 + -3 * X3 + -6 * X4 + -6 * X5\n"
          ]
        }
      ],
      "source": [
        "random_coeff = jax.random.randint(key, shape=[num_features], minval=-10, maxval=10)\n",
        "\n",
        "X = 2 * jax.random.normal(key, shape=(num_samples, num_features))\n",
        "\n",
        "# Generate Random Bias and Coefficients\n",
        "random_bais = jax.random.choice(key, random_coeff, shape=(1,))\n",
        "random_coeff = jax.random.choice(key, random_coeff, shape=(num_features,))\n",
        "\n",
        "print(f\"Random Bias: {random_bais}\")\n",
        "print(f\"Random Coefficients: {random_coeff}\")\n",
        "\n",
        "coeff_features = []\n",
        "\n",
        "# Construct each feature with random coeffcients choosen\n",
        "for idx, coeff in enumerate(random_coeff):\n",
        "  coeff_features.append(coeff * X[:, idx:idx+1])\n",
        "\n",
        "# Print equation\n",
        "equation = f\"Y = {random_bais[0]}\"\n",
        "for idx, coeff in enumerate(random_coeff):\n",
        "  equation += f\" + {coeff} * X{idx+1}\"\n",
        "print(equation)\n",
        "\n",
        "\n",
        "# Stack the features into a single matrix\n",
        "coeff_features = jnp.hstack(coeff_features)\n",
        "\n",
        "# Generate output from random data\n",
        "y = random_bais + jnp.sum(coeff_features, axis=1) + jax.random.normal(key, shape=(num_samples,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puAisRQKdIWE"
      },
      "source": [
        "## Normal Equation\n",
        "\n",
        "$$\\theta = (X^TX)^{-1} X^Ty$$\n",
        "\n",
        "* The Normal Equation computes the inverse of $X^T X$, which is an $(n + 1) × (n + 1)$\n",
        "matrix (where n is the number of features). The computational complexity of inverting\n",
        "such a matrix is typically about $O(n^{2.4})$ to $O(n^3)$ (depending on the implementation).\n",
        "In other words, if you double the number of features, you multiply the computation\n",
        "time by roughly 22.4 = 5.3 to 23 = 8\n",
        "\n",
        "* $X^TX$ is always guaranteed to be symmetric and positive semi-definite (non-negative eigenvalues), but that doesn't necessarily mean it's invertible.\n",
        "  * **Linear Depedence:** $X^TX$ not being invertible implies the columns (or rows, since it's symmetric) of the original matrix $X$ are linearly dependent. This means one column can be recreated by a linear combination of other columns. Imagine $X$ has redundant information.\n",
        "  * **Non-Unique Solutions:**  In contexts like linear regression where $X^TX$ is used to find a best-fit line, non-invertibility indicates there isn't a single unique solution for the unknown variables. There might be multiple lines that fit the data about equally well.\n",
        "  * **Rank and Interpretation:** The rank of $X$, which is essentially the number of linearly independent columns (or rows), is less than the total number of columns (or rows) if $X^TX$ is not invertible. This can point to issues with the data or model being used.\n",
        "\n",
        "* Even when $X^TX$ is not invertible, alternative methods like the Moore-Penrose pseudoinverse can be used to find solutions, although they might not be unique.\n",
        "\n",
        "* Both the Normal Equation and the SVD approach get very slow\n",
        "when the number of features grows large (e.g., 100,000). On the\n",
        "positive side, both are linear with regards to the number of instances\n",
        "in the training set (they are $O(m)$), so they handle large training\n",
        "sets efficiently, provided they can fit in memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wlxjgcJ1i-UA"
      },
      "outputs": [],
      "source": [
        "class NormFit:\n",
        "    \"\"\"\n",
        "    A class for fitting and predicting using the normal equation.\n",
        "\n",
        "    Attributes:\n",
        "        theta (numpy.ndarray): The parameters of the linear regression model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the NormFit class with theta set to None.\n",
        "        \"\"\"\n",
        "        self.theta = None\n",
        "\n",
        "    def fit(self, X: jnp.ndarray, y: jnp.ndarray)-> jnp.ndarray:\n",
        "        \"\"\"\n",
        "        Fit the linear regression model using the normal equation.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): The input features.\n",
        "            y (numpy.ndarray): The target values.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: The learned parameters (theta).\n",
        "        \"\"\"\n",
        "        X_b = jnp.hstack((jnp.ones((X.shape[0], 1)), X))  # Add a column of ones for the bias term\n",
        "        self.theta = jnp.linalg.pinv(X_b).dot(y)  # Use np.linalg.pinv instead of np.linalg.inv for better numerical stability\n",
        "        return self.theta\n",
        "\n",
        "    def predict(self, X: jnp.ndarray)-> jnp.ndarray:\n",
        "        \"\"\"\n",
        "        Predict the target values using the learned model.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): The input features.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: The predicted target values.\n",
        "        \"\"\"\n",
        "        return X.dot(self.theta)  # Corrected: Use self.theta instead of theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtrYEzV8tSQ3"
      },
      "source": [
        "### Fit Normal Equation to Data to get Weights/Coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxSq_nXodPpO",
        "outputId": "b5aa949e-f8a1-4734-d5f2-138ce386915a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Y_ = -6.0 + -3.0 * X1 + -1.0 * X2 + -3.0 * X3 + -6.0 * X4 + -6.0 * X5\n"
          ]
        }
      ],
      "source": [
        "normfit = NormFit()\n",
        "theta = normfit.fit(X, y)\n",
        "\n",
        "predicted_eq = f\"Y_ = {jnp.round(theta[0], 1)}\"\n",
        "for idx, coeff in enumerate(theta[1:]):\n",
        "  predicted_eq += f\" + {jnp.round(coeff, 1)} * X{idx+1}\"\n",
        "\n",
        "print(predicted_eq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTtZx9VfrgOb"
      },
      "source": [
        "## Gradient Descent Training\n",
        "\n",
        "\n",
        "* Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.\n",
        "* Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly is to go downhill in the direction of the steepest slope.\n",
        "* This is exactly what Gradient Descent does: **it measures the local gradient of the error function with regards to the parameter vector θ, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum!**\n",
        "* An important parameter in Gradient Descent is the size of the steps, determined by the `learning rate` hyperparameter.\n",
        "  * If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time.\n",
        "  * On the other hand, if the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before. This might make the algorithm diverge, with larger and larger values, failing to find a good solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AtXC3DUQmLa"
      },
      "source": [
        "## Linear Regression Gradient Descent:\n",
        "\n",
        "$$ MSE(\\theta) = \\dfrac{1}{m} * \\sum_{i=1}^{m} (\\theta^T x^{(i)} - y^{i})^{2}$$\n",
        "\n",
        "$$ \\ $$\n",
        "$$ \\dfrac{\\partial}{\\partial \\theta_j}MSE(\\theta) = \\dfrac{2}{m} * \\sum_{i=1}^{m} (\\theta^Tx^{(i)} - y^{(i)}) \\, \\, x_{j}^{(i)}$$\n",
        "\n",
        "$$ \\ $$\n",
        "\n",
        "$$ \\nabla_\\theta MSE(\\theta) = \\left\\{ \\begin{array}{c}\n",
        "\\frac{\\partial}{\\partial \\theta_0} \\text{MSE}(\\theta) \\\\\n",
        "\\frac{\\partial}{\\partial \\theta_1} \\text{ MSE}(\\theta) \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial}{\\partial \\theta_n} \\text{MSE}(\\theta)\n",
        "\\end{array} \\right\\} = \\dfrac{2}{m}X^{T}(X\\theta - y)$$\n",
        "\n",
        "$$ \\ $$\n",
        "\n",
        "$$ \\theta^{(next \\, step)} = \\theta - 𝞰\\nabla_\\theta MSE(\\theta) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aRKKmN_Wrf09"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "class LinearRegression:\n",
        "    \"\"\"\n",
        "    Linear Regression model implementation using JAX.\n",
        "\n",
        "    Attributes:\n",
        "        key (jnp.ndarray): Random key for weight initialization.\n",
        "        lr (float): Learning rate for gradient descent.\n",
        "        epochs (int): Number of training epochs.\n",
        "        W (jnp.ndarray): Weights of the linear regression model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, key, lr: float = 0.01, epochs: int = 1000):\n",
        "        self.key = key\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.W: jnp.ndarray = None\n",
        "\n",
        "    def fit(self, X: jnp.ndarray, y: jnp.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Fit the linear regression model to the training data.\n",
        "\n",
        "        Args:\n",
        "            X (jnp.ndarray): Input features of shape (num_samples, num_features).\n",
        "            y (jnp.ndarray): Target values of shape (num_samples,) or (num_samples, 1).\n",
        "        \"\"\"\n",
        "        num_features = X.shape[1]\n",
        "        self.W = jax.random.normal(self.key, shape=(num_features + 1, 1))  # (num_features + bias term, 1)\n",
        "        X = jnp.hstack((jnp.ones(shape=(X.shape[0], 1)), X))  # (add column for bias term which will be one)\n",
        "        y = y.reshape(-1, 1)  # Ensure y has shape (num_samples, 1)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Forward Pass\n",
        "            y_pred = X @ self.W  # (num_samples, num_features + 1) @ (num_features + 1, 1) -> (num_samples, 1)\n",
        "\n",
        "            # Calculate Gradient\n",
        "            dW = (2 / X.shape[0]) * (X.T @ (y_pred - y))  # (num_features + 1, num_samples) @ (num_samples, 1) -> (num_features + 1, 1)\n",
        "\n",
        "            # Update Weights\n",
        "            self.W -= self.lr * dW\n",
        "\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                loss = jnp.sum((y_pred - y) ** 2)\n",
        "                print(f\"Epoch {epoch + 1}: Loss {loss}\")\n",
        "\n",
        "    def predict(self, X: jnp.ndarray) -> jnp.ndarray:\n",
        "        \"\"\"\n",
        "        Predict the target values for the given input features.\n",
        "\n",
        "        Args:\n",
        "            X (jnp.ndarray): Input features of shape (num_samples, num_features).\n",
        "\n",
        "        Returns:\n",
        "            jnp.ndarray: Predicted target values of shape (num_samples, 1).\n",
        "        \"\"\"\n",
        "        X = jnp.hstack((jnp.ones(shape=(X.shape[0], 1)), X))  # Add bias term\n",
        "        return X @ self.W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzkyzNCD-Dfv"
      },
      "source": [
        "### Fit the Dataset using Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mAo9r8Bx3Jk5"
      },
      "outputs": [],
      "source": [
        "lr = LinearRegression(key, lr=0.01, epochs=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdl_csTu6YO2",
        "outputId": "fa02fb17-7dae-4504-ee63-f20f4289631a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100: Loss 1889.560791015625\n",
            "Epoch 200: Loss 1053.281982421875\n",
            "Epoch 300: Loss 1038.103271484375\n",
            "Epoch 400: Loss 1037.8277587890625\n",
            "Epoch 500: Loss 1037.82275390625\n",
            "Epoch 600: Loss 1037.82275390625\n",
            "Epoch 700: Loss 1037.8226318359375\n",
            "Epoch 800: Loss 1037.8226318359375\n",
            "Epoch 900: Loss 1037.8226318359375\n",
            "Epoch 1000: Loss 1037.8226318359375\n"
          ]
        }
      ],
      "source": [
        "lr.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZiAu2tp-WaJ"
      },
      "source": [
        "### Check the coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln3suTXm983S",
        "outputId": "1ba4d3a2-b708-4afa-c540-439af0fab3e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Y_ = -6.0 + -3.0 * X1 + -1.0 * X2 + -3.0 * X3 + -6.0 * X4 + -6.0 * X5\n"
          ]
        }
      ],
      "source": [
        "predicted_eq = f\"Y_ = {jnp.round(lr.W[0, 0], 1)}\"\n",
        "for idx, coeff in enumerate(lr.W[1:, 0]):\n",
        "  predicted_eq += f\" + {jnp.round(coeff, 1)} * X{idx+1}\"\n",
        "\n",
        "print(predicted_eq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYa3t53LL0uD"
      },
      "source": [
        "# **TODO:**\n",
        "1. More About Convex functions\n",
        "2. Train using one any open dataset\n",
        "3. Evalualte $R^2$ Error, Adjusted $R^2$\n",
        "4. Check for p-value from the statistical significance of the coefficients\n",
        "5. Train for multi target output from Scratch\n",
        "6. Significance of Metrics\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
